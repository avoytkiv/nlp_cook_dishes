{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use `fastText` pretrained word vectors (Mikolov et al., 2017), trained on 600 billion tokens on Common Crawl. fastText is an upgraded version of word2vec and outperforms other state-of-the-art methods by a large margin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-11-04 04:33:37--  https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M.vec.zip\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 3.163.189.108, 3.163.189.14, 3.163.189.51, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|3.163.189.108|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1523785255 (1.4G) [application/zip]\n",
      "Saving to: ‘fastText/crawl-300d-2M.vec.zip’\n",
      "\n",
      "crawl-300d-2M.vec.z   6%[>                   ]  87.87M   146MB/s               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crawl-300d-2M.vec.z 100%[===================>]   1.42G   186MB/s    in 7.3s    \n",
      "\n",
      "2023-11-04 04:33:44 (199 MB/s) - ‘fastText/crawl-300d-2M.vec.zip’ saved [1523785255/1523785255]\n",
      "\n",
      "Archive:  fastText/crawl-300d-2M.vec.zip\n",
      "  inflating: fastText/crawl-300d-2M.vec  \n",
      "CPU times: user 373 ms, sys: 119 ms, total: 492 ms\n",
      "Wall time: 43.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "URL = \"https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M.vec.zip\"\n",
    "FILE = \"fastText\"\n",
    "\n",
    "if os.path.isdir(FILE):\n",
    "    print(\"fastText exists.\")\n",
    "else:\n",
    "    !wget -P $FILE $URL\n",
    "    !unzip $FILE/crawl-300d-2M.vec.zip -d $FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using the CPU instead.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():       \n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "    print('Device name:', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "\n",
    "To prepare our text data for training, first we need to tokenize our sentences and build a vocabulary dictionary `word2idx`, which will later be used to convert our tokens into indexes and build an embedding layer.\n",
    "\n",
    "An embedding layer serves as a look-up table which takes words’ indexes in the vocabulary as input and output word vectors. Hence, the embedding layer has shape `\\((N, d)\\)` where `\\(N\\)` is the size of the vocabulary and `\\(d\\)` is the embedding dimension. In order to fine-tune pretrained word vectors, we need to create an embedding layer in our nn.Module class. Our input to the model will then be `input_ids`, which is tokens’ indexes in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "\n",
    "\n",
    "def clean_steps(data):\n",
    "    data_list = ast.literal_eval(data)\n",
    "    return ', '.join(data_list)\n",
    "\n",
    "\n",
    "BOS, EOS = ' ', '\\n'\n",
    "data = pd.read_csv('RAW_recipes.csv')\n",
    "data = data[['name', 'steps', 'description', 'ingredients']]\n",
    "data = data[~data['steps'].str.contains('please ignore')]\n",
    "data = data[~data['steps'].str.contains('none')]\n",
    "data['steps'] = data['steps'].apply(clean_steps)\n",
    "data['ingredients'] = data['ingredients'].apply(clean_steps)\n",
    "\n",
    "lines = data.apply(lambda row: str(row['name']) + \n",
    "                   '; ' + \n",
    "                   str(row['steps']).replace(\"\\n\", ' ') + \n",
    "                   '; ' + \n",
    "                   str(row['description']) + \n",
    "                   '; ' + \n",
    "                   str(row['ingredients'])[:512], axis=1) \\\n",
    "            .apply(lambda line: BOS + line.replace(EOS, ' ') + EOS) \\\n",
    "            .tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
